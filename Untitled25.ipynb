{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMCN6JJ2rsO1RPz/LIBwa2E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lHZdAYP7nO57"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","\n","# -------------------------------\n","# 1. DATA LOADING\n","# -------------------------------\n","# Example synthetic dataset\n","def generate_dataset(n_samples=2000):\n","    t = np.arange(n_samples)\n","    data = pd.DataFrame({\n","        \"value1\": np.sin(0.02 * t) + np.random.normal(0, 0.1, n_samples),\n","        \"value2\": np.cos(0.02 * t) + np.random.normal(0, 0.1, n_samples),\n","        \"target\": np.sin(0.02 * t + 1) + np.random.normal(0, 0.1, n_samples)\n","    })\n","    return data\n","\n","data = generate_dataset()\n","\n","# -------------------------------\n","# 2. PREPROCESSING\n","# -------------------------------\n","features = data[[\"value1\", \"value2\"]].values\n","target = data[\"target\"].values.reshape(-1, 1)\n","\n","scaler_x = StandardScaler()\n","scaler_y = StandardScaler()\n","\n","features = scaler_x.fit_transform(features)\n","target = scaler_y.fit_transform(target)\n","\n","# -------------------------------\n","# 3. CREATE SEQUENCE WINDOWS\n","# -------------------------------\n","SEQ_LEN = 30  # look-back window\n","\n","def create_sequences(x, y, seq_len):\n","    xs, ys = [], []\n","    for i in range(len(x) - seq_len):\n","        xs.append(x[i:i+seq_len])\n","        ys.append(y[i+seq_len])\n","    return np.array(xs), np.array(ys)\n","\n","X, y = create_sequences(features, target, SEQ_LEN)\n","\n","# Train/Val/Test split\n","train_size = int(0.7 * len(X))\n","val_size = int(0.15 * len(X))\n","\n","X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n","y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n","\n","# -------------------------------\n","# 4. PYTORCH DATASET\n","# -------------------------------\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.tensor(X, dtype=torch.float32)\n","        self.y = torch.tensor(y, dtype=torch.float32)\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=32, shuffle=True)\n","val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=32)\n","\n","# -------------------------------\n","# 5. TRANSFORMER MODEL\n","# -------------------------------\n","class TransformerModel(nn.Module):\n","    def __init__(self, feature_size, num_heads=2, num_layers=2):\n","        super().__init__()\n","        self.encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=feature_size,\n","            nhead=num_heads,\n","            dropout=0.1,\n","            dim_feedforward=128\n","        )\n","        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        self.fc = nn.Linear(feature_size, 1)\n","\n","    def forward(self, src):\n","        out = self.transformer(src)     # (seq_len, batch, features)\n","        out = out[-1, :, :]             # Take last time step\n","        return self.fc(out)\n","\n","model = TransformerModel(feature_size=2)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# -------------------------------\n","# 6. TRAINING LOOP\n","# -------------------------------\n","def train(model, train_loader, val_loader):\n","    for epoch in range(20):\n","        model.train()\n","        for X_batch, y_batch in train_loader:\n","            X_batch = X_batch.permute(1, 0, 2)   # (seq, batch, feature)\n","            pred = model(X_batch)\n","            loss = criterion(pred, y_batch)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            X_val_b, y_val_b = next(iter(val_loader))\n","            pred_val = model(X_val_b.permute(1, 0, 2))\n","            val_loss = criterion(pred_val, y_val_b)\n","\n","        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n","\n","train(model, train_loader, val_loader)\n","\n","# -------------------------------\n","# 7. TEST EVALUATION\n","# -------------------------------\n","model.eval()\n","X_test_t = torch.tensor(X_test, dtype=torch.float32).permute(1, 0, 2)\n","with torch.no_grad():\n","    predictions = model(X_test_t).numpy()\n","\n","# Inverse scale\n","predictions = scaler_y.inverse_transform(predictions)\n","y_test_inv = scaler_y.inverse_transform(y_test)\n","\n","# Metrics\n","mae = mean_absolute_error(y_test_inv, predictions)\n","rmse = np.sqrt(mean_squared_error(y_test_inv, predictions))\n","\n","print(\"\\nðŸ“Œ TEST RESULTS\")\n","print(\"MAE:\", mae)\n","print(\"RMSE:\", rmse)\n","\n","# -------------------------------\n","# 8. PLOT RESULTS\n","# -------------------------------\n","plt.figure(figsize=(12, 5))\n","plt.plot(y_test_inv[:200], label=\"Actual\")\n","plt.plot(predictions[:200], label=\"Predicted\")\n","plt.legend()\n","plt.title(\"Transformer Forecasting\")\n","plt.show()\n"]}]}